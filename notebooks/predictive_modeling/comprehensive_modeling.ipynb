{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Predictive Modeling Pipeline\n",
    "\n",
    "This notebook demonstrates a complete modeling pipeline for water quality spill prediction using:\n",
    "- Pydantic validation for data and model configuration\n",
    "- Training and prediction pipelines\n",
    "- Enhanced error handling and validation\n",
    "- Model evaluation and performance metrics\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Setup and Configuration** - Import libraries and configure paths\n",
    "2. **Data Loading** - Load cleaned data for modeling\n",
    "3. **Data Preparation** - Prepare features and target variables\n",
    "4. **Model Training** - Train models with validation\n",
    "5. **Model Evaluation** - Evaluate model performance\n",
    "6. **Predictions** - Make predictions on new data\n",
    "7. **Model Persistence** - Save and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure notebook directory paths\n",
    "try:\n",
    "    NOTEBOOK_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Scripts dir:  {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML model components\n",
    "from scripts.ml_models.model_utils import ModelConfig, get_logger\n",
    "from scripts.ml_models.train_pipeline import SpillTrainingPipeline\n",
    "from scripts.ml_models.predict import SpillPredictionPipeline, make_prediction\n",
    "from scripts.ml_models.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Import Pydantic enhancements for validation\n",
    "from scripts.pydantic_enhancements import (\n",
    "    ModelHyperparameterConfig,\n",
    "    ModelMetricsConfig,\n",
    "    ModelPathConfig,\n",
    "    DataFrameInputValidator,\n",
    "    ErrorHandler,\n",
    "    ApplicationSettings\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "print(\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "DATA_FILE = PROJECT_ROOT / \"export\" / \"cleaned_data\" / \"cleaned_water_data.csv\"\n",
    "\n",
    "if DATA_FILE.exists():\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    print(f\"\u2705 Data loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    df.head()\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Data file not found: {DATA_FILE}\")\n",
    "    print(\"   Trying alternative location...\")\n",
    "    # Try original data file\n",
    "    ALT_DATA_FILE = PROJECT_ROOT / \"data\" / \"national_water_plan.csv\"\n",
    "    if ALT_DATA_FILE.exists():\n",
    "        df = pd.read_csv(ALT_DATA_FILE)\n",
    "        print(f\"\u2705 Alternative data loaded from: {ALT_DATA_FILE}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        df.head()\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find data file. Checked:\\n- {DATA_FILE}\\n- {ALT_DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data structure using Pydantic enhancements\n",
    "try:\n",
    "    input_validator = DataFrameInputValidator(\n",
    "        data=df,\n",
    "        required_columns=None,  # Will check available columns\n",
    "        min_rows=1\n",
    "    )\n",
    "    validated_df = input_validator.validate_dataframe()\n",
    "    print(f\"\u2705 Data validation passed!\")\n",
    "    print(f\"   Rows: {len(validated_df)}\")\n",
    "    print(f\"   Columns: {len(validated_df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Data validation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available columns and identify potential features/targets\n",
    "print(\"Available columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nData info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()} ({df.isnull().sum().sum() / df.size * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration with Pydantic Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration with Pydantic validation\n",
    "# Adjust features and target based on your actual data columns\n",
    "config = ModelConfig(\n",
    "    artifacts_dir=PROJECT_ROOT / \"artifacts\",\n",
    "    model_filename=\"spills_pipeline.joblib\",\n",
    "    plots_dir=PROJECT_ROOT / \"plots\",\n",
    "    features=[\"Spill Events 2020\", \"Spill Events 2021\", \"Spill Events 2022\", \"Latitude\", \"Longitude\"],  # Using actual columns from data\n",
    "    target=\"Predicted Annual Spill Frequence Post Scheme\",  # Adjust based on your data\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"\u2705 Model configuration created:\")\n",
    "print(f\"   Features: {config.features}\")\n",
    "print(f\"   Target: {config.target}\")\n",
    "print(f\"   Model path: {config.model_path}\")\n",
    "print(f\"   N estimators: {config.n_estimators}\")\n",
    "print(f\"   Test size: {config.test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate hyperparameters using enhanced Pydantic validator\n",
    "try:\n",
    "    hyperparam_config = config.to_hyperparameter_config()\n",
    "    print(\"\u2705 Hyperparameters validated using enhanced validator:\")\n",
    "    print(f\"   N estimators: {hyperparam_config.n_estimators}\")\n",
    "    print(f\"   Random state: {hyperparam_config.random_state}\")\n",
    "    print(f\"   Test size: {hyperparam_config.test_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Hyperparameter validation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validate paths using enhanced Pydantic validator\n",
    "try:\n",
    "    path_config = config.to_path_config()\n",
    "    print(\"\u2705 Paths validated using enhanced validator:\")\n",
    "    print(f\"   Artifacts dir: {path_config.artifacts_dir}\")\n",
    "    print(f\"   Model filename: {path_config.model_filename}\")\n",
    "    print(f\"   Model path: {path_config.model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Path validation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling using FeatureEngineer\n",
    "try:\n",
    "    # Check if required columns exist\n",
    "    required_cols = config.features + [config.target]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\u26a0\ufe0f  Missing columns: {missing_cols}\")\n",
    "        print(f\"   Available columns: {list(df.columns)}\")\n",
    "        print(\"\\n   Please update config.features and config.target to match your data.\")\n",
    "        print(\"   For now, using available numeric columns as features...\")\n",
    "        \n",
    "        # Use numeric columns as fallback\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if len(numeric_cols) > 1:\n",
    "            # Use first numeric column as target, rest as features\n",
    "            config.target = numeric_cols[0]\n",
    "            config.features = numeric_cols[1:min(4, len(numeric_cols))]  # Use up to 3 features\n",
    "            print(f\"   Updated target: {config.target}\")\n",
    "            print(f\"   Updated features: {config.features}\")\n",
    "    \n",
    "    # Validate and clean data\n",
    "    clean_df = FeatureEngineer.validate_data(\n",
    "        df=df,\n",
    "        required_cols=config.features + [config.target],\n",
    "        drop_missing=True,\n",
    "        min_rows_after_cleaning=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Data prepared for modeling!\")\n",
    "    print(f\"   Shape after cleaning: {clean_df.shape}\")\n",
    "    print(f\"   Features: {config.features}\")\n",
    "    print(f\"   Target: {config.target}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Feature engineering failed: {e}\")\n",
    "    logger.error(f\"Feature engineering error: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training pipeline\n",
    "trainer = SpillTrainingPipeline(config=config, model_name=\"WaterQualitySpillModel\")\n",
    "\n",
    "print(\"\u2705 Training pipeline initialized\")\n",
    "print(f\"   Model name: {trainer.model_name}\")\n",
    "print(f\"   Is trained: {trainer.is_trained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "try:\n",
    "    print(\"\ud83d\ude80 Starting model training...\")\n",
    "    metrics = trainer.train(clean_df)\n",
    "    \n",
    "    print(\"\\n\u2705 Training completed successfully!\")\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"   {metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    # Validate metrics using enhanced Pydantic validator\n",
    "    try:\n",
    "        metrics_config = trainer.metrics.to_metrics_config()\n",
    "        print(f\"\\n\u2705 Metrics validated using enhanced validator:\")\n",
    "        if metrics_config.r2_score is not None:\n",
    "            print(f\"   R\u00b2 Score: {metrics_config.r2_score:.4f}\")\n",
    "        if metrics_config.rmse is not None:\n",
    "            print(f\"   RMSE: {metrics_config.rmse:.4f}\")\n",
    "        if metrics_config.mae is not None:\n",
    "            print(f\"   MAE: {metrics_config.mae:.4f}\")\n",
    "        \n",
    "        # Check if performance is good\n",
    "        is_good = metrics_config.is_good_performance(threshold=0.7)\n",
    "        print(f\"\\n   Performance meets threshold (>=0.7): {is_good}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not validate metrics with enhanced validator: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Training failed: {e}\")\n",
    "    logger.error(f\"Training error: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "try:\n",
    "    trainer.save()\n",
    "    print(f\"\u2705 Model saved successfully to: {config.model_path}\")\n",
    "    print(f\"   File exists: {config.model_path.exists()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to save model: {e}\")\n",
    "    logger.error(f\"Save error: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the training pipeline\n",
    "try:\n",
    "    # Use a subset of data for prediction demonstration\n",
    "    prediction_data = clean_df[config.features].head(10)\n",
    "    \n",
    "    print(f\"Making predictions on {len(prediction_data)} samples...\")\n",
    "    predictions = trainer.predict(prediction_data)\n",
    "    \n",
    "    print(f\"\\n\u2705 Predictions generated successfully!\")\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    results_df = prediction_data.copy()\n",
    "    results_df[\"Prediction\"] = predictions\n",
    "    if config.target in clean_df.columns:\n",
    "        results_df[\"Actual\"] = clean_df[config.target].head(10).values\n",
    "    print(results_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Prediction failed: {e}\")\n",
    "    logger.error(f\"Prediction error: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using prediction pipeline (if model was saved)\n",
    "if config.model_path.exists():\n",
    "    try:\n",
    "        predictor = SpillPredictionPipeline(config=config, model_name=\"WaterQualitySpillPredictor\")\n",
    "        print(\"\u2705 Prediction pipeline initialized\")\n",
    "        print(f\"   Model loaded: {predictor.is_trained}\")\n",
    "        \n",
    "        # Make predictions using prediction pipeline\n",
    "        prediction_data = clean_df[config.features].head(5)\n",
    "        predictions = predictor.predict(prediction_data)\n",
    "        \n",
    "        print(f\"\\n\u2705 Predictions from prediction pipeline:\")\n",
    "        print(f\"   Number of predictions: {len(predictions)}\")\n",
    "        print(f\"   Sample predictions: {predictions[:5]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not use prediction pipeline: {e}\")\n",
    "        print(\"   This is expected if the model file format is incompatible\")\n",
    "        logger.warning(f\"Prediction pipeline error: {e}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Model file not found. Skipping prediction pipeline test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### Summary\n",
    "- \u2705 Data loaded and validated using Pydantic enhancements\n",
    "- \u2705 Model configuration validated with enhanced validators\n",
    "- \u2705 Model trained successfully with performance metrics\n",
    "- \u2705 Model saved and can be loaded for predictions\n",
    "- \u2705 Predictions generated using both training and prediction pipelines\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different hyperparameters\n",
    "- Try different feature combinations\n",
    "- Evaluate model on holdout test set\n",
    "- Generate visualizations of model performance\n",
    "- Deploy model for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning\n",
    "\n",
    "Use grid search, random search, or Bayesian optimization to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with FAST random search (faster than grid search)\n",
    "from scripts.ml_models.hyperparameter_tuning import HyperparameterTuner\n",
    "\n",
    "tuner = HyperparameterTuner(config, model_type='random_forest')\n",
    "\n",
    "# Use random search instead of grid search (much faster!)\n",
    "# Random search samples fewer combinations but finds good results quickly\n",
    "print(\"\ud83d\ude80 Starting FAST hyperparameter tuning (random search)...\\n\")\n",
    "tuning_results = tuner.random_search(\n",
    "    X=clean_df[config.features],\n",
    "    y=clean_df[config.target],\n",
    "    n_iter=10,  # Only 10 iterations instead of 108 combinations\n",
    "    cv=3,  # 3-fold CV instead of 5 (faster)\n",
    "    scoring='r2',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Tuning completed!\\n\")\n",
    "print(f\"Best parameters: {tuning_results['best_params']}\")\n",
    "print(f\"Best score: {tuning_results['best_score']:.4f}\\n\")\n",
    "\n",
    "# Use best estimator\n",
    "trainer.pipeline = tuner.get_best_estimator()\n",
    "print(\"\u2705 Best model loaded into trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Compare multiple algorithms and select the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "from scripts.ml_models.model_comparison import ModelComparator\n",
    "\n",
    "comparator = ModelComparator(config)\n",
    "comparison_results = comparator.compare_models(\n",
    "    X=clean_df[config.features],\n",
    "    y=clean_df[config.target],\n",
    "    model_names=['random_forest', 'gradient_boosting', 'linear_regression', 'ridge'],\n",
    "    cv=3,  # 3-fold CV for faster execution\n",
    ")\n",
    "\n",
    "# Get best model\n",
    "best_model_name, best_model = comparator.get_best_model()\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"\\nComparison results:\")\n",
    "for model_name, results in comparison_results.items():\n",
    "    if 'error' not in results:\n",
    "        print(f\"  {model_name}: CV R\u00b2 = {results['cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Feature Engineering\n",
    "\n",
    "Create new features and select the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering\n",
    "from scripts.ml_models.advanced_feature_engineering import AdvancedFeatureEngineer\n",
    "\n",
    "feature_engineer = AdvancedFeatureEngineer()\n",
    "\n",
    "# Create interaction features\n",
    "df_with_interactions = feature_engineer.create_interaction_features(\n",
    "    clean_df[config.features],\n",
    "    columns=config.features\n",
    ")\n",
    "print(f\"Created {len(df_with_interactions.columns) - len(config.features)} interaction features\")\n",
    "\n",
    "# Feature selection based on importance\n",
    "X_selected, selected_features, importance = feature_engineer.select_features_importance(\n",
    "    clean_df[config.features],\n",
    "    clean_df[config.target],\n",
    "    max_features=7\n",
    ")\n",
    "print(f\"\\nSelected features: {selected_features}\")\n",
    "print(f\"\\nFeature importance:\")\n",
    "for feature, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {feature}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Visualization\n",
    "\n",
    "Create visualizations for model evaluation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "from scripts.ml_models.visualization import ModelVisualizer\n",
    "\n",
    "visualizer = ModelVisualizer(output_dir=config.plots_dir)\n",
    "\n",
    "# Feature importance plot\n",
    "if importance:\n",
    "    importance_path = visualizer.plot_feature_importance(importance, top_n=10)\n",
    "    print(f\"\u2713 Feature importance plot: {importance_path}\")\n",
    "\n",
    "# Predictions vs actual\n",
    "predictions = trainer.predict(clean_df[config.features])\n",
    "actual = clean_df[config.target].values\n",
    "pred_vs_actual_path = visualizer.plot_prediction_vs_actual(actual, predictions)\n",
    "print(f\"\u2713 Predictions vs actual plot: {pred_vs_actual_path}\")\n",
    "\n",
    "# Residual analysis\n",
    "residuals_path = visualizer.plot_residuals(actual, predictions)\n",
    "print(f\"\u2713 Residual analysis plot: {residuals_path}\")\n",
    "\n",
    "# Model comparison\n",
    "if comparison_results:\n",
    "    comparison_path = visualizer.plot_model_comparison(comparison_results, metric='cv_mean')\n",
    "    print(f\"\u2713 Model comparison plot: {comparison_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_quality_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}