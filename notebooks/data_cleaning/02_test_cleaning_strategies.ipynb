{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 · Test Cleaning Strategies\n",
        "\n",
        "Use this notebook to prototype different `DataCleanerConfig` combinations before wiring them into automated jobs. Each run reuses the production cleaner so results mirror what would happen in `scripts/data-cleaner.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook goals\n",
        "\n",
        "- Quickly compare multiple cleaning strategies without editing production code\n",
        "- Capture row-retention and missing-value metrics per strategy\n",
        "- Surface trade-offs (strict vs. relaxed) for stakeholders\n",
        "- Provide reusable helper functions for future experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import importlib.util\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
        "DATA_PATH = PROJECT_ROOT / \"data\" / \"national_water_plan.csv\"\n",
        "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
        "\n",
        "\n",
        "def load_module(module_name: str, file_path: Path):\n",
        "    if module_name in sys.modules:\n",
        "        return sys.modules[module_name]\n",
        "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[module_name] = module\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "\n",
        "\n",
        "data_loader = load_module(\"project_data_loader\", SCRIPTS_DIR / \"data-loader.py\")\n",
        "DataLoader = data_loader.DataLoader\n",
        "DataConfig = data_loader.DataConfig\n",
        "\n",
        "data_cleaner = load_module(\"project_data_cleaner\", SCRIPTS_DIR / \"data-cleaner.py\")\n",
        "DataCleanerConfig = data_cleaner.DataCleanerConfig\n",
        "WaterDataCleaner = data_cleaner.WaterDataCleaner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data & build a working sample\n",
        "\n",
        "The sample keeps experiments fast while still flowing through the exact same cleaning methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_config = DataConfig(filepath=str(DATA_PATH))\n",
        "loader = DataLoader(data_config)\n",
        "raw_ddf, exploration_report = loader.load_and_explore_data()\n",
        "\n",
        "SAMPLE_ROWS = 20_000\n",
        "sample_pdf = raw_ddf.head(SAMPLE_ROWS, compute=True)\n",
        "print(f\"Exploration rows: {exploration_report.metadata.rows:,}\")\n",
        "print(f\"Sample size: {len(sample_pdf):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CleaningReport = data_cleaner.CleaningReport\n",
        "\n",
        "@dataclass\n",
        "class StrategyResult:\n",
        "    name: str\n",
        "    description: str\n",
        "    config: DataCleanerConfig\n",
        "    report: CleaningReport\n",
        "    cleaned_ddf: \"dd.DataFrame\"\n",
        "\n",
        "\n",
        "def run_cleaning_strategy(name: str, description: str, overrides: dict) -> StrategyResult:\n",
        "    base_kwargs = dict(\n",
        "        strict_mode=True,\n",
        "        remove_duplicates=True,\n",
        "        remove_outliers=True,\n",
        "        remove_invalid_spill_years=True,\n",
        "        remove_invalid_text_values=True,\n",
        "        create_backup=False,\n",
        "        save_cleaning_report=False,\n",
        "        output_directory=\"/tmp\",\n",
        "    )\n",
        "    base_kwargs.update(overrides)\n",
        "    config = DataCleanerConfig(**base_kwargs)\n",
        "    cleaner = WaterDataCleaner(config)\n",
        "    cleaned_ddf, report = cleaner.clean_data(sample_pdf.copy(), output_dir=None)\n",
        "    return StrategyResult(name, description, config, report, cleaned_ddf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define strategy grid\n",
        "\n",
        "Tweak just a handful of parameters per strategy so we can reason about the outcome of each change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strategies = [\n",
        "    {\n",
        "        \"name\": \"strict_default\",\n",
        "        \"description\": \"Baseline configuration with strict column requirements and duplicate removal.\",\n",
        "        \"overrides\": {},\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"relaxed_text\",\n",
        "        \"description\": \"Allow rows that miss optional text fields while keeping numeric validation strict.\",\n",
        "        \"overrides\": {\n",
        "            \"remove_invalid_text_values\": False,\n",
        "            \"missing_value_threshold\": 0.4,\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"spill_imputation\",\n",
        "        \"description\": \"Fill missing spill values with zeroes and keep rows even if only two years are available.\",\n",
        "        \"overrides\": {\n",
        "            \"fill_missing_values\": True,\n",
        "            \"fill_value\": 0,\n",
        "            \"remove_invalid_spill_years\": False,\n",
        "            \"min_valid_spill_years\": 2,\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"aggressive_outliers\",\n",
        "        \"description\": \"Tighten outlier filtering to 2σ and demand four valid spill years.\",\n",
        "        \"overrides\": {\n",
        "            \"outlier_std_threshold\": 2.0,\n",
        "            \"min_valid_spill_years\": 4,\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results: list[StrategyResult] = []\n",
        "for strat in strategies:\n",
        "    result = run_cleaning_strategy(strat[\"name\"], strat[\"description\"], strat[\"overrides\"])\n",
        "    results.append(result)\n",
        "    print(f\"✓ {strat['name']} completed — rows retained: {result.report.cleaned_shape[0]:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare outcomes\n",
        "\n",
        "Aggregate the metrics that matter most (row retention, missing %, duplicates removed) and visualize trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_records = []\n",
        "for result in results:\n",
        "    metrics = result.report.quality_metrics\n",
        "    summary_records.append(\n",
        "        {\n",
        "            \"strategy\": result.name,\n",
        "            \"description\": result.description,\n",
        "            \"rows_in\": result.report.original_shape[0],\n",
        "            \"rows_out\": result.report.cleaned_shape[0],\n",
        "            \"rows_retained_pct\": metrics.get(\"rows_retained_percent\"),\n",
        "            \"initial_missing_pct\": metrics.get(\"initial_missing_percent\"),\n",
        "            \"final_missing_pct\": metrics.get(\"final_missing_percent\"),\n",
        "            \"duplicates_removed\": metrics.get(\"duplicate_reduction\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.barplot(data=summary_df, x=\"strategy\", y=\"rows_retained_pct\", ax=axes[0], color=\"#3182bd\")\n",
        "axes[0].set_title(\"Row retention %\")\n",
        "axes[0].set_ylim(0, 105)\n",
        "\n",
        "sns.barplot(data=summary_df, x=\"strategy\", y=\"final_missing_pct\", ax=axes[1], color=\"#e6550d\")\n",
        "axes[1].set_title(\"Final missing %\")\n",
        "axes[1].set_ylim(0, summary_df[\"final_missing_pct\"].max() * 1.2)\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_xlabel(\"Strategy\")\n",
        "    ax.tick_params(axis=\"x\", rotation=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removal breakdown per strategy\n",
        "\n",
        "Inspect which rules are driving the drop in rows to tune thresholds precisely.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "removal_records = []\n",
        "for result in results:\n",
        "    breakdown = result.report.removal_breakdown or {}\n",
        "    for reason, count in breakdown.items():\n",
        "        removal_records.append(\n",
        "            {\n",
        "                \"strategy\": result.name,\n",
        "                \"reason\": reason,\n",
        "                \"rows_removed\": count,\n",
        "            }\n",
        "        )\n",
        "\n",
        "if removal_records:\n",
        "    breakdown_df = (\n",
        "        pd.DataFrame(removal_records)\n",
        "        .pivot_table(index=\"reason\", columns=\"strategy\", values=\"rows_removed\", fill_value=0)\n",
        "        .reindex(columns=summary_df[\"strategy\"], fill_value=0)\n",
        "        .sort_index(ascending=False)\n",
        "    )\n",
        "else:\n",
        "    breakdown_df = pd.DataFrame()\n",
        "\n",
        "breakdown_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Peek at cleaned sample for a chosen strategy\n",
        "\n",
        "Use this helper to inspect the head/tail of the cleaned Dask DataFrame (converted to pandas for convenience).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preview_strategy(strategy_name: str, n: int = 5):\n",
        "    match = next((r for r in results if r.name == strategy_name), None)\n",
        "    if match is None:\n",
        "        raise ValueError(f\"Strategy '{strategy_name}' not found. Choose from {[r.name for r in results]}\")\n",
        "    preview_df = match.cleaned_ddf.head(n, compute=True)\n",
        "    display(preview_df)\n",
        "\n",
        "\n",
        "preview_strategy(\"strict_default\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "\n",
        "- Use the summary table to document how each config impacts retention vs. quality.\n",
        "- Feed promising overrides back into `DataCleanerConfig` defaults or environment-specific settings.\n",
        "- Commit strategy descriptions so future analysts understand _why_ thresholds changed.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
