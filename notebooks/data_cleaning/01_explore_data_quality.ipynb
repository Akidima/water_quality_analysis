{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Â· Explore Data Quality\n",
        "\n",
        "This optional enhancement notebook inspects the raw National Water Plan dataset before any cleaning happens. It mirrors the guidance in `docs/DEVELOPMENT_RECOMMENDATIONS.md` by focusing on the diagnostic steps that inform how `scripts/data-cleaner.py` should be configured.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook goals\n",
        "\n",
        "- Understand baseline quality metrics (shape, missingness, duplicates)\n",
        "- Inspect geographic coordinates to confirm valid ranges\n",
        "- Review spill event coverage to surface outliers or data gaps\n",
        "- Explore categorical health indicators for text-heavy columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
        "pd.set_option(\"display.max_columns\", 40)\n",
        "pd.set_option(\"display.precision\", 2)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
        "DATA_PATH = PROJECT_ROOT / \"data\" / \"national_water_plan.csv\"\n",
        "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
        "\n",
        "\n",
        "def load_module(module_name: str, file_path: Path):\n",
        "    if module_name in sys.modules:\n",
        "        return sys.modules[module_name]\n",
        "\n",
        "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[module_name] = module\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "\n",
        "\n",
        "data_loader = load_module(\"project_data_loader\", SCRIPTS_DIR / \"data-loader.py\")\n",
        "DataLoader = data_loader.DataLoader\n",
        "DataConfig = data_loader.DataConfig\n",
        "ExplorationReport = data_loader.ExplorationReport\n",
        "\n",
        "data_cleaner = load_module(\"project_data_cleaner\", SCRIPTS_DIR / \"data-cleaner.py\")\n",
        "WaterDataCleaner = data_cleaner.WaterDataCleaner\n",
        "DataCleanerConfig = data_cleaner.DataCleanerConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load raw dataset with the production loader\n",
        "\n",
        "The `DataLoader` handles schema validation, dtype optimization, and produces the same `ExplorationReport` structure the cleaning pipeline expects. Keeping the exploratory view aligned with production logic makes it easier to cross-reference findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_config = DataConfig(filepath=str(DATA_PATH))\n",
        "loader = DataLoader(data_config)\n",
        "raw_ddf, exploration_report = loader.load_and_explore_data()\n",
        "\n",
        "raw_preview = raw_ddf.head(5)\n",
        "raw_preview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline metadata\n",
        "\n",
        "Use the exploration report to understand the big-picture dimensions and high-level warnings before diving into column-level diagnostics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = exploration_report.metadata\n",
        "print(\n",
        "    f\"Rows: {metadata.rows:,}\\n\"\n",
        "    f\"Columns: {metadata.columns}\\n\"\n",
        "    f\"Memory (MB): {metadata.memory_usage:.2f}\\n\"\n",
        "    f\"Missing (%): {metadata.missing_values_percent:.2f}\\n\"\n",
        "    f\"Duplicate rows: {metadata.duplicate_rows:,}\"\n",
        ")\n",
        "\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        \"type\": [\"errors\", \"warnings\"],\n",
        "        \"messages\": [exploration_report.errors, exploration_report.warnings],\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a manageable in-memory sample\n",
        "\n",
        "Large Dask frames are great for scale, but plotting and quick profiling is smoother with a pandas sample. Adjust `SAMPLE_ROWS` if you want a deeper slice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE_ROWS = 10_000\n",
        "sample_pdf = raw_ddf.head(SAMPLE_ROWS, compute=True)\n",
        "\n",
        "print(f\"Sample rows: {len(sample_pdf):,} (first partitions only)\")\n",
        "sample_pdf.describe(include=\"all\").transpose().head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Missing values and duplicates\n",
        "\n",
        "Quantify missingness for critical fields to prioritize cleaning rules. The `DataCleanerConfig` uses these same column lists as guardrails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_config = DataCleanerConfig()\n",
        "key_columns = [col for col in sorted(set(base_config.required_columns + base_config.optional_columns)) if col in raw_ddf.columns]\n",
        "\n",
        "missing_counts = (\n",
        "    raw_ddf[key_columns]\n",
        "    .isnull()\n",
        "    .sum()\n",
        "    .compute()\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "missing_df = (\n",
        "    pd.DataFrame({\"missing\": missing_counts})\n",
        "    .assign(percent=lambda df: (df[\"missing\"] / metadata.rows) * 100)\n",
        "    .sort_values(\"percent\", ascending=False)\n",
        ")\n",
        "\n",
        "duplicate_series = raw_ddf.map_partitions(\n",
        "    lambda part: pd.Series({\"duplicates\": int(part.duplicated().sum())}),\n",
        "    meta=pd.Series(dtype=\"int64\", name=\"duplicates\"),\n",
        ")\n",
        "duplicate_count = int(duplicate_series.sum().compute())\n",
        "\n",
        "print(f\"Duplicate rows detected: {duplicate_count:,}\")\n",
        "missing_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "(\n",
        "    missing_df.head(12)[::-1]\n",
        "    .plot(kind=\"barh\", y=\"percent\", legend=False, ax=ax, color=\"#3182bd\")\n",
        ")\n",
        "ax.set_xlabel(\"Missing (%)\")\n",
        "ax.set_ylabel(\"Column\")\n",
        "ax.set_title(\"Top missing columns (raw data)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Geographic coordinate diagnostics\n",
        "\n",
        "Latitude/longitude validation removes invalid locations. Plotting a quick scatter exposes obvious geographic outliers before defining stricter bounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coord_cols = [col for col in [\"Latitude\", \"Longitude\"] if col in raw_ddf.columns]\n",
        "coord_summary = (\n",
        "    raw_ddf[coord_cols]\n",
        "    .describe(percentiles=[0.25, 0.5, 0.75])\n",
        "    .compute()\n",
        "    .loc[[\"count\", \"min\", \"max\", \"mean\", \"std\"]]\n",
        ")\n",
        "\n",
        "lat_range = (base_config.lat_min, base_config.lat_max)\n",
        "lon_range = (base_config.lon_min, base_config.lon_max)\n",
        "\n",
        "invalid_lat = (\n",
        "    (~raw_ddf[\"Latitude\"].between(lat_range[0], lat_range[1]))\n",
        "    .sum()\n",
        "    .compute()\n",
        "    if \"Latitude\" in raw_ddf.columns\n",
        "    else 0\n",
        ")\n",
        "invalid_lon = (\n",
        "    (~raw_ddf[\"Longitude\"].between(lon_range[0], lon_range[1]))\n",
        "    .sum()\n",
        "    .compute()\n",
        "    if \"Longitude\" in raw_ddf.columns\n",
        "    else 0\n",
        ")\n",
        "\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        \"metric\": [\"latitude\", \"longitude\"],\n",
        "        \"invalid_rows\": [invalid_lat, invalid_lon],\n",
        "        \"min_config\": [lat_range[0], lon_range[0]],\n",
        "        \"max_config\": [lat_range[1], lon_range[1]],\n",
        "    }\n",
        "), coord_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geo_sample = sample_pdf.dropna(subset=coord_cols)\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "ax.scatter(\n",
        "    geo_sample[\"Longitude\"],\n",
        "    geo_sample[\"Latitude\"],\n",
        "    s=10,\n",
        "    alpha=0.4,\n",
        "    linewidths=0,\n",
        "    color=\"#31a354\",\n",
        ")\n",
        "ax.set_title(\"Sample geographic coverage (raw)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.axhline(lat_range[0], color=\"red\", linestyle=\"--\", linewidth=0.8)\n",
        "ax.axhline(lat_range[1], color=\"red\", linestyle=\"--\", linewidth=0.8)\n",
        "ax.axvline(lon_range[0], color=\"orange\", linestyle=\"--\", linewidth=0.8)\n",
        "ax.axvline(lon_range[1], color=\"orange\", linestyle=\"--\", linewidth=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Spill event coverage\n",
        "\n",
        "These columns often drive downstream analytics. Understanding missingness and value spread makes it easier to tune outlier handling and the `min_valid_spill_years` rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spill_cols = [col for col in base_config.spill_year_columns if col in raw_ddf.columns]\n",
        "\n",
        "spill_stats = (\n",
        "    raw_ddf[spill_cols]\n",
        "    .describe(percentiles=[0.5, 0.75, 0.9])\n",
        "    .compute()\n",
        "    .loc[[\"count\", \"mean\", \"std\", \"min\", \"50%\", \"75%\", \"90%\", \"max\"]]\n",
        "    .transpose()\n",
        "    .rename(columns={\"50%\": \"median\"})\n",
        ")\n",
        "\n",
        "missing_spill = (\n",
        "    raw_ddf[spill_cols]\n",
        "    .isnull()\n",
        "    .sum()\n",
        "    .compute()\n",
        ")\n",
        "\n",
        "spill_stats = spill_stats.assign(\n",
        "    missing=missing_spill,\n",
        "    missing_pct=lambda df: (df[\"missing\"] / metadata.rows) * 100,\n",
        ")\n",
        "\n",
        "spill_stats.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yearly_totals = (\n",
        "    raw_ddf[spill_cols]\n",
        "    .sum()\n",
        "    .compute()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"index\": \"year\", 0: \"total_events\"})\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "sns.barplot(\n",
        "    data=yearly_totals,\n",
        "    x=\"year\",\n",
        "    y=\"total_events\",\n",
        "    color=\"#6baed6\",\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Spill event totals by year (raw)\")\n",
        "ax.set_xlabel(\"Year column\")\n",
        "ax.set_ylabel(\"Events\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text field health check\n",
        "\n",
        "Whitespace trimming plus emptiness checks remove rows that lack contextual metadata. Review the dominant values to confirm expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_cols = [col for col in base_config.text_columns if col in sample_pdf.columns]\n",
        "text_summary = []\n",
        "\n",
        "for col in text_cols:\n",
        "    series = sample_pdf[col].astype(str).str.strip()\n",
        "    series = series.replace({\"\": pd.NA, \"nan\": pd.NA})\n",
        "    missing = series.isna().sum()\n",
        "    top_value = series.mode().iloc[0] if not series.mode().empty else None\n",
        "    top_freq = int((series == top_value).sum()) if top_value else 0\n",
        "    text_summary.append(\n",
        "        {\n",
        "            \"column\": col,\n",
        "            \"missing_pct\": missing / len(series) * 100,\n",
        "            \"unique\": series.nunique(dropna=True),\n",
        "            \"top_value\": top_value,\n",
        "            \"top_freq\": top_freq,\n",
        "        }\n",
        "    )\n",
        "\n",
        "pd.DataFrame(text_summary).sort_values(\"missing_pct\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "\n",
        "- Feed the missing value table into `DataCleanerConfig` thresholds (`missing_value_threshold`, `min_valid_spill_years`).\n",
        "- Adjust coordinate ranges before running the pipeline if geographic outliers look legitimate.\n",
        "- Use the spill event totals to calibrate outlier handling (`outlier_std_threshold`).\n",
        "- Keep the text frequency table handy when reviewing rows that get dropped for empty metadata.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "water_quality_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
