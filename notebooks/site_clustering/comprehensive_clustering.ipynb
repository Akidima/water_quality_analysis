{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83334455",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "\n",
    "# Use a non-interactive backend automatically when running headless (CI/servers)\n",
    "if os.environ.get(\"DISPLAY\", \"\") == \"\" and os.environ.get(\"MPLBACKEND\", \"\") == \"\":\n",
    "    matplotlib.use(\"Agg\") # Must be set before importing pyplot\n",
    "import joblib\n",
    "import logging\n",
    "from dataclasses import dataclass, field \n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, cast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Set up logging - like a diary\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(names)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ClusteringConfig:\n",
    "    \"\"\"\n",
    "\n",
    "    Configuration for clustering analysis.\n",
    "\n",
    "    Think of this like: A settings menu in a video game where you choose\n",
    "    diffculty level and other options before playing.\n",
    "\n",
    "    \"\"\"\n",
    "    # File Paths\n",
    "    output_dir: str = 'plots'\n",
    "    model_save_path: str = 'ml_models/clustering_model.pkl'\n",
    "    scaler_save_path: str = 'ml_models/clustering_scaler.pkl'\n",
    "\n",
    "    # Clustering Parameters\n",
    "    min_clusters: int = 2 # Minimum number of groups\n",
    "    max_clusters: int = 8 # Maximum number of groups\n",
    "    random_state: int = 42 # For reproducibility (consistency)\n",
    "    n_init: int = 10 # How many times to try clustering\n",
    "    max_iter: int = 300 # Maximum iterations per attempt\n",
    "\n",
    "    # Feature selection\n",
    "    base_features: List[str] = field(default_factory=lambda: [\n",
    "        'Latitude',\n",
    "        'Longitude',\n",
    "        'Avg_Annual_Spills'\n",
    "    ])\n",
    "\n",
    "    optional_features: List[str] = field(default_factory=lambda: [\n",
    "        'Spill Trend',\n",
    "        'Predicted Annual Spill Frequence Post Scheme',\n",
    "        'Ecological High Priority Site Flag',\n",
    "        'Non-bathing Priority Site Flag',\n",
    "        'Bathing Water Discharge Flag',\n",
    "        'Shellfish Water Discharge Flag'\n",
    "    ])\n",
    "\n",
    "    # Visualization\n",
    "    figure_size: Tuple[int, int] = (16, 12)\n",
    "    dpi: int = 300\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create directories after initalization.\"\"\"\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.model_save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.scaler_save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_feature_list(self) -> List[str]:\n",
    "        \"\"\"Return a list of all features to use in clustering.\"\"\"\n",
    "        return self.base_features + self.optional_features\n",
    "    \n",
    "class ClusterValidator:\n",
    "    \"\"\"\n",
    "    Validates clustering results to ensure they're good quality.\n",
    "\n",
    "    Think of this like: A quality inspector who checks if your toy sorting\n",
    "    actually makes sense - are similar toys really grouped together?\n",
    "\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def calcuate_cluster_metrics(\n",
    "        X: np.ndarray,\n",
    "        labels: np.ndarray\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calcuate how good the clustering is.\n",
    "\n",
    "        Args:\n",
    "            X: The feature data\n",
    "            labels: Cluster assignments\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of quality scores\n",
    "        \n",
    "        Think of this like: Getting a report card for your sorting job!\n",
    "        \"\"\"\n",
    "        n_clusters = len(np.unique(labels))\n",
    "\n",
    "        # Skip metrics if only 1 cluster (no comparsion possible)\n",
    "        if n_clusters < 2:\n",
    "            return {\n",
    "                'silhouette_score': 0.0,\n",
    "                'davies_bouldin_score': 0.0,\n",
    "                'calinski_harabasz_score': 0.0\n",
    "            }\n",
    "        try:\n",
    "            # Silhouette Score: -1 to 1 (higher is better)\n",
    "            # Measures how similar objects are to their own cluster compared to others own\n",
    "            silhouette = silhouette_score(X, labels)\n",
    "\n",
    "            # Davies-Bouldin Score: 0 to infinity (lower is better)\n",
    "            # Measures how much overlap there is between clusters\n",
    "            db_score = davies_bouldin_score(X, labels)\n",
    "\n",
    "            # Calinski-Harabasz Score: 0 to infinity (higher is better)\n",
    "            # Measures how much clusters are well-separated from each other\n",
    "            ch_score = calinski_harabasz_score(X, labels)\n",
    "\n",
    "            return {\n",
    "                'silhouette_score': silhouette,\n",
    "                'davies_bouldin_score': db_score,\n",
    "                'calinski_harabasz_score': ch_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e: \n",
    "            logger.warning(f\"Error calcuating cluster metrics: {e}\")\n",
    "            return {\n",
    "                'silhouette_score': 0.0,\n",
    "                'davies_bouldin_score': 0.0,\n",
    "                'calinski_harabasz_score': 0.0\n",
    "            }\n",
    "\n",
    "    @staticmethod\n",
    "    def interpret_metrics(metrics: Dict[str, float]) -> str:\n",
    "        \"\"\"\n",
    "\n",
    "        Explain what the metrics mean in plain English.\n",
    "\n",
    "        Think of this like: Your teacher explaining what your grades mean.\n",
    "        \"\"\"\n",
    "        silhouette = metrics['silhouette_score']\n",
    "\n",
    "        if silhouette > 0.7:\n",
    "            quality = \"Excellent - Clusters are very distinct and well-seperated\"\n",
    "        elif silhouette > 0.5:\n",
    "            quality = \"Good - Clusters are reasonably well-defined\"\n",
    "        elif silhouette > 0.3:\n",
    "            quality = \"Fair - Clusters have some overlap\"\n",
    "        else:\n",
    "            quality = \"Poor - Clusters are not well-defined\"\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    class OptimalClusterFinder:\n",
    "        \"\"\"\n",
    "        Finds the best number of clusters automatically.\n",
    "\n",
    "        Think of this like: A smart helper that figures out exactly how many\n",
    "        toy bins you need - not too many, not too few, just right!\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, config: ClusteringConfig):\n",
    "            self.config = config\n",
    "            self.inertias = []\n",
    "            self.k_range = range(config.min_clusters, config.max_clusters + 1)\n",
    "        \n",
    "        def find_optimal_k(\n",
    "            self,\n",
    "            X: np.ndarray\n",
    "        ) -> Tuple[int, List[float]]:\n",
    "            \"\"\"\n",
    "            Use the elbow method to find optimal number of clusters.\n",
    "\n",
    "            Args:\n",
    "                X: Scaled feature data\n",
    "            \n",
    "            Returns:\n",
    "                optimal_k: Best number of clusters\n",
    "                inertias: List of inertia values for each k\n",
    "\n",
    "            Think of this like: Trying different numbers of toys bins and finding\n",
    "            the sweet spot where organization is best!\n",
    "            \"\"\"\n",
    "            logger.info(\"Finding optimal number of clusters using elbow method\")\n",
    "\n",
    "            self.inertias = []\n",
    "\n",
    "            # Try different number of clusters\n",
    "            for k in self.k_range:\n",
    "                kmeans = KMeans(\n",
    "                    n_clusters=k,\n",
    "                    random_state=self.config.random_state,\n",
    "                    n_init=self.config.n_init,  # type: ignore\n",
    "                    max_iter=self.config.max_iter\n",
    "                )\n",
    "                kmeans.fit(X)\n",
    "                self.inertias.append(kmeans.inertia_)\n",
    "\n",
    "            # Plot elbow curve\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(self.k_range, self.inertias, marker='o', linestyle='--')\n",
    "            \n",
    "            # Use Kneelocator to find the \"elbow\" point automatically\n",
    "            try:\n",
    "                knee_locator = KneeLocator(\n",
    "                    list(self.k_range),\n",
    "                    self.inertias,\n",
    "                    curve='convex',\n",
    "                    direction='decreasing'\n",
    "                )\n",
    "                optimal_k = knee_locator.elbow\n",
    "\n",
    "                # If no clear elbow found, use middle value\n",
    "                if optimal_k is None:\n",
    "                    optimal_k = (self.config.min_clusters + self.config.max_clusters) // 2\n",
    "                    logger.warning(f\"No clear data found, using k={optimal_k}\")\n",
    "                \n",
    "                else:\n",
    "                    logger.info(f\"Optimal k found: {optimal_k}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Fallback to middle value if knee detection fails\n",
    "                optimal_k = (self.config.min_clusters + self.config.max_clusters) // 2\n",
    "                logger.warning(f\"Knee detection failed: {e}. Using k={optimal_k}\")\n",
    "\n",
    "            return optimal_k, self.inertias\n",
    "        \n",
    "        class SiteClusterer:\n",
    "            \"\"\"\n",
    "            Main class for clustering water quality monitoring sites.\n",
    "\n",
    "            Think of this like: The master organizer who takes all your toys\n",
    "            and sort them into neat, logical groups\n",
    "            \"\"\"\n",
    "\n",
    "            def __init__(self, config: Optional[ClusteringConfig] = None):\n",
    "                \"\"\"Initalize the clusterer.\"\"\"\n",
    "                self.config = config or ClusteringConfig()\n",
    "                self.model = None\n",
    "                self.scaler = None\n",
    "                self.featuree_names = []\n",
    "                self.cluster_analysis = {}\n",
    "                self.optimal_k = None\n",
    "            \n",
    "            def prepare_features(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "                \"\"\"\n",
    "                Prepare and select features for clustering.\n",
    "\n",
    "                Think of this like: Picking out which characteristics of your toys\n",
    "                to use for sorting (color, size, type, etc.)\n",
    "\n",
    "                Args:\n",
    "                    df: Input dataframe\n",
    "\n",
    "                Returns:\n",
    "                    DataFrame with selected features, or None if validation fails\n",
    "                \"\"\"\n",
    "                # Check base required columns\n",
    "                missing_base = [col for col in self.config.base_features if col not in df.columns]\n",
    "                if missing_base:\n",
    "                    logger.warning(f\"Missing required base features: {missing_base}. Exiting.\")\n",
    "                    return None\n",
    "                \n",
    "                # Start with base features\n",
    "                selected_features = self.config.base_features.copy()\n",
    "\n",
    "                # Add optional features if available\n",
    "                for feature in self.config.optional_features:\n",
    "                    if feature in df.columns:\n",
    "                        selected_features.append(feature)\n",
    "                        logger.info(f\"Added optinal feature: {feature}\")\n",
    "                \n",
    "                # Extract features - ensure it's a DataFrame (not Series)\n",
    "                # Using cast to tell type checker this is always a DataFrame\n",
    "                feature_df = cast(pd.DataFrame, df[selected_features].copy())\n",
    "\n",
    "                # Convert flag columns to integers if they exist\n",
    "                flag_columns = [col for col in feature_df.columns if 'Flag' in col]\n",
    "                for col in flag_columns:\n",
    "                    feature_df[col] = feature_df[col].astype(int)\n",
    "\n",
    "                # Handle missing values with median (more robust than mean)\n",
    "                feature_df = feature_df.fillna(feature_df.median())\n",
    "\n",
    "                # Store feature names\n",
    "                self.feature_names = feature_df.columns.tolist()\n",
    "\n",
    "                # Log feature selection\n",
    "                logger.info(f\"Selected features: {len(self.feature_names)} features: {self.feature_names}\")\n",
    "\n",
    "                return feature_df\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_quality_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
